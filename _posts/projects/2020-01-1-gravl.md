---
layout: post
title: Ground Robotic Autonomous Vehicle Lab
subtitle: Developing an autonomous tractor as a research platform
image: /img/projects/gravl/tractor-square.jpg
share-img: /img/projects/gravl/tractor-square.jpg
tags: [Olin-Robotics-Lab]
---
The Ground Robotic Autonomous Vehicle Lab (GRAVL) is a student-run research group at Olin that I've done research with since my first year at Olin. During my first year, we set our sights on working on research that would help to forward development towards automated dirt road leveling. Seeing that technology towards self-driving cars has been rapidly advancing in urban environments over the last few years, we envisioned a future in which these vehicles would be capable of navigating off-road areas or other unstructured environments as well. In order to achieve this vision, we believe that our work will contribute to the existing tech now so that rural roads will be ready when the time for widespread adoption of autonomous vehicles comes.

During my time on the team, I've worked on a mix of electro-mechanical system development (Freshman Year), simulation development (Sophomore Year), and most recently using machine learning for object detection and classification in 3D pointclouds (Junior Year). For more details, keep scrolling!

Project Links:
+ [Main Repository](https://github.com/olinrobotics/gravl)
+ [Simulation](https://github.com/olinrobotics/tractor_sim)
+ Other Maintained Repositories
   + [Robot State Controller](https://github.com/olinrobotics/state_controller)
+ [Wiki](https://github.com/olinrobotics/gravl/wiki)


### Junior Year 2019 (Fall Only):
During my junior year, the team participated in the
[Panasonic Prototype 3D Lidar challenge](https://www.massrobotics.org/2020/05/21/panasonic-prototype-3d-lidar-challenge/)
and were selected as a finalist, receiving $2.5K for project funding and a loaned prototype 3D LiDAR to complete our project. I worked on a sub-project aimed at exploring the possibility of training datasets in simulation or with standard datasets data to use with the 3D Lidar, so we started with training a model on the Waymo dataset. Our results are presented in the image below:

<center>
  {% include overlay.html
    file="/img/projects/gravl/ml.jpg"
    padding-top="5px"
    width="95%"
    id="ml"%}
</center>

[Source Code](https://github.com/AmyPhung/ml_person_detection)


Other platform improvements completed during this year are summarized in this video
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://www.youtube.com/embed/sytteTSZ4Ug' frameborder='0' allowfullscreen></iframe></div>

### Sophomore Year 2018-2019:
After my first year experience, we quickly learned that our heavy reliance on a physical platform had a number of issues: namely, only one person could test code at any given point, systems were co-dependent and tests would be halted due to one failing system, and most importantly, the ground freezing during the winter months hindered our ability to test and effectively prevented us from testing during the end of the Fall semester and the first half of the Spring semester. During this year, the group refocused to robust platform development to improve upon the issues we encountered during our first year. Noticing our issues with extremely slow testing cycles and weather-dependent testing, I worked on developing a simulation that could be used as the first test platform. This simulation allowed us to massively decrease our testing cycles, allowing tests that had previously required several team members to run a 3 hour field mission to now allowing tests to be completed by an individual within an hour of debugging in simulation.

The source code to the simulation can be viewed [here](https://github.com/olinrobotics/tractor_sim)

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://www.youtube.com/embed/raW6wDYDRLI' frameborder='0' allowfullscreen></iframe></div>

### Freshman Year 2017-2018:
When I joined the team during my first semester, the group was in the middle of developing a package delivery system with a combination of an UAV (Unmanned Aerial Vehicle) and an UGV (Unmanned Ground Vehicle) as an "Air-Amazon" for US Marines. This semester's work primarily consisted of individual sub-system development and testing. Demonstrated behaviors include 2D Lidar-based target tracking and path planning, autonomous offboard control of a Pixhawk-based drone, and monocular camera-based unstructured road detection.

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://www.youtube.com/embed/e5cIqTJsXIo' frameborder='0' allowfullscreen></iframe></div>

In my second semester, since we were no longer under a contract, we chose to pursue a new project related to automated dirt road maintenance & terraforming. Although our tractor was equipped with a three-point hitch that could be fitted with a box blade to terraform dirt, it was not currently outfitted for autonomous operation. My primary contribution this semester was in developing the  firmware and low-level actuator control and design a custom solution for precise actuation and control of the vehicleâ€™s hitch. At the conclusion of this semester, we were able to detect bumps and divots in dirt by scanning the ground with a 2D lidar and precisely control the height of the box blade through ROS.

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://www.youtube.com/embed/rLdPoTPenYo' frameborder='0' allowfullscreen></iframe></div>
