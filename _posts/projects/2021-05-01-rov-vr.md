---
layout: post
title: ROV-VR
subtitle: Leveraging virtual reality to improve underwater robotic vehicle piloting for deep-sea scientific sampling
image: /img/projects/rov-vr/rov-vr-cover-img.png
share-img: /img/projects/rov-vr/rov-vr-cover-img.png
tags: [Olin-SCOPE]
---

For my senior capstone project at Olin, I led the development of a control room display in virtual reality (VR) to help improve operation of underwater remotely operated vehicles (ROVs). Thanks to generous support from the Dassault Systèmes Foundation, we were able to work on this project in collaboration with the Monterey Bay Aquarium Research Institute (MBARI) over the course of the academic year.

<center>
  <a href="/files/rov-vr/mbari-poster.pdf" target="_blank">
    <img src="/img/projects/rov-vr/poster-preview.jpg" style="width:95%;" >
  </a>
  <br/>
  (Click for full-resolution PDF)
</center>

## Background & Motivation
Typical ROV control rooms consist of a wall of fixed monitors, each displaying a separate piece of telemetry data. Here’s what the current control room looks like

<center>
  {% include overlay.html
    file="/img/projects/rov-vr/control-room.jpg"
    padding-top="5px"
    width="80%"
    id="control-room"%}
</center>

Seated at the center of the displays is the pilot who controls the ROV, and to their right is the scientist who identifies targets of scientific interest. Successfully collecting a sample requires seamless communication between the pilot and scientist, but the existing control room makes this challenging.

In conversations with one of the scientists, they noted that they’ve been using laser pointers to point at objects on the screen to communicate with the pilots, which has a tendency to reflect off of the TV screens into everyone's eyes. Some additional pain points of the existing control room include the fact that the TV monitors are quite bulky and can’t be reconfigured easily, and that the current control room requires a lot of skill and training to operate due to the significant cognitive load it poses.

Our objective was to create a VR control room prototype to explore the potential of VR for alleviating these pain points and improving collaboration between scientists and ROV pilots.

## Our Project
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://www.youtube.com/embed/inJNb7pWUso' frameborder='0' allowfullscreen></iframe></div>

We built our app in Unity, and used [Lightweight Communications and Marshalling (LCM)](https://github.com/lcm-proj/lcm) to read live telemetry data from the ROV. Our project also uses live footage from the newly added 180°, 4K stereo-pair fisheye camera onboard the ROV and computes the reprojections necessary to create an immersive, 1:1 scale, 3D view of the ROV’s surroundings in VR. Providing ROV pilots with a wide field of view, without the distortion that results from projecting raw footage onto a flat screen, should improve pilots’ spatial awareness during operation.

<center>
  {% include overlay.html
    file="/img/projects/rov-vr/simulated-vr-view.gif"
    padding-top="5px"
    width="90%"
    id="simulated-view"%}
    <br/>
    Visual representation of the stereo footage that
    <br/>
    produces a 3D view in a VR headset
</center>

To begin incorporating the functionality of the existing control room into our application, our app incorporates the real-time telemetry data into the 3D environment and adds multi-user support to allow pilots and scientists to collaborate in VR.

<center>
  {% include overlay.html
    file="/img/projects/rov-vr/pilot-scientist.jpg"
    padding-top="5px"
    width="80%"
    id="pilot-scientist"%}
    <br/>
    Desktop app allows seamless communication
    <br/>
    while pilot is wearing VR headset
</center>

Our app also includes hand and gaze-based controls that enable pilots to rearrange the displays, a feature not possible with fixed monitors.

<center>
  {% include overlay.html
    file="/img/projects/rov-vr/handsui.gif"
    padding-top="5px"
    width="50%"
    id="handsui"%}
    <br/>
    Our VR app allows pilots to quickly
    <br/>
    rearrange displays with their hands
</center>

Since pilots have individual preferences for how these displays are arranged, we included an option to save their display preferences, allowing for seamless transitions between pilots.

<center>
  {% include overlay.html
    file="/img/projects/rov-vr/settings-menu.png"
    padding-top="5px"
    width="80%"
    id="settings-menu"%}
  <br/>
  Pop-up settings menu and LCM data debugger
</center>

We demonstrated our application during the operation of MBARI’s MiniROV in a 1 million liter saltwater test tank facility and had several ROV pilots try out our app. The response was fantastic - most of the pilots we spoke to said they were extremely excited to test it out at sea later in the upcoming summer!

<center>
  {% include overlay.html
    file="/img/projects/rov-vr/tank-test-pilot.jpeg"
    padding-top="5px"
    width="35%"
    id="tank-test-pilot"%}
  {% include overlay.html
    file="/img/projects/rov-vr/tank-test-rov.jpeg"
    padding-top="5px"
    width="62%"
    id="tank-test-rov"%}
</center>

My primary technical contributions to this project included the stereo reprojection shader, implementing the stereo-camera-to-Unity pipeline, implementing the hand, controller, and foot/gaze based controls, and creating the LCM debugging window.

## Next Steps
Although this may be the conclusion of our work, this project will continue to live on - MBARI plans to take our app to sea for further testing during the summer, and a research group at UC Santa Cruz is looking to do in-depth user studies with our app to quantify whether or not our app actually reduces task completion time. Other avenues of improvement include integrating advanced features like MBARI’s ongoing work on machine-learning based organism tracking, and continuing to make other general UI revisions. There’s also discussion at MBARI about potentially using the results from this testing to create a control room designed around VR for their new ship that will eventually replace the one used for their ROV Doc Ricketts.

## See Also
+ <a href="https://www.lafondation3ds.org/projects/enhancing-ocean-discovery-and-exploration" target="_blank">
   Article from the Dassault Systèmes Foundation about our project
  </a>
+ <a href="https://www.olin.edu/collaborate/scope/projects/2020-21/MBARI-Dassault-Foundation" target="_blank">
   Olin SCOPE project listing
  </a>
+ <a href="http://ccscne.org/conferences/ccscne-2021/" target="_blank">
   See the abstract and poster we presented at CCSCNE 2021!
  </a>
